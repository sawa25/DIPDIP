{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Чтение данных\n",
    "events = pd.read_csv(\"dfev.csv\")\n",
    "properties = pd.concat([\n",
    "    pd.read_csv(\"item_properties_part1.csv.zip\"),\n",
    "    pd.read_csv(\"item_properties_part2.csv.zip\")\n",
    "])\n",
    "categories = pd.read_csv(\"category_tree.csv\")\n",
    "\n",
    "# Добавление новых признаков\n",
    "events['event_datetime'] = pd.to_datetime(events['timestamp'], unit='ms')\n",
    "properties['event_datetime'] = pd.to_datetime(properties['timestamp'], unit='ms')\n",
    "events['day_of_week'] = events['event_datetime'].map(lambda x: x.weekday())\n",
    "events['Year'] = events['event_datetime'].map(lambda x: x.year)\n",
    "events['Month'] = events['event_datetime'].map(lambda x: x.month)\n",
    "events['Day'] = events['event_datetime'].map(lambda x: x.day)\n",
    "events['Hour'] = events['event_datetime'].map(lambda x: x.hour)\n",
    "events['minute'] = events['event_datetime'].map(lambda x: x.minute)\n",
    "\n",
    "def get_time_periods(hour):\n",
    "    if hour >= 3 and hour < 7:\n",
    "        return 'Dawn'\n",
    "    elif hour >= 7 and hour < 12:\n",
    "        return 'Morning'\n",
    "    elif hour >= 12 and hour < 16:\n",
    "        return 'Afternoon'\n",
    "    elif hour >= 16 and hour < 22:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "events['Day Period'] = events['Hour'].map(get_time_periods)\n",
    "\n",
    "# Фильтрация данных\n",
    "transaction_events = events[events['event'] == 'transaction']\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "train, test = train_test_split(transaction_events, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Предобработка данных\n",
    "top_properties = properties.drop_duplicates(['itemid', 'property']).groupby(\"property\")['itemid'].count().sort_values(ascending=False)[:20]\n",
    "\n",
    "properties_filtered = properties[properties['property'].isin(set(top_properties.index))]\n",
    "\n",
    "# Преобразование свойств товаров с буквенно-цифровыми значениями, разделенными пробелами, в числовые признаки\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "vectorizer = CountVectorizer()\n",
    "properties_vectorized = vectorizer.fit_transform(properties_filtered['value'][:10])\n",
    "properties_encoded = encoder.fit_transform(properties_vectorized.toarray())\n",
    "properties_df = pd.DataFrame(properties_encoded, columns=encoder.get_feature_names_out())\n",
    "properties_df['itemid'] = properties_filtered['itemid'][:10]\n",
    "properties_df\n",
    "\n",
    "\n",
    "# Создаём dataframe, содержащий все itemid и их свойства\n",
    "item_properties = properties_df.groupby('itemid').mean().reset_index()\n",
    "\n",
    "# Формируем данные для модели\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "train_data = Dataset.load_from_df(train[['visitorid', 'itemid']], reader)\n",
    "test_data = Dataset.load_from_df(test[['visitorid', 'itemid']], reader)\n",
    "\n",
    "# Создаём и обучаем модель\n",
    "svd = SVD()\n",
    "cross_validate(svd, train_data, measures=['RMSE', 'MAE', 'Precision@3'], cv=3, verbose=True)\n",
    "svd.fit(train_data.build_full_trainset())\n",
    "\n",
    "# Получаем факторную матрицу для товаров\n",
    "item_factors = svd.qi.T\n",
    "\n",
    "# Добавляем информацию о свойствах товаров к факторной матрице\n",
    "item_factors_with_properties = np.hstack((item_factors, item_properties.drop('itemid', axis=1).values))\n",
    "\n",
    "# Нормализуем полученную матрицу\n",
    "item_factors_with_properties_normalized = item_factors_with_properties / np.linalg.norm(item_factors_with_properties, axis=1)[:, np.newaxis]\n",
    "\n",
    "# Вычисляем косинусное сходство между товарами\n",
    "item_similarity = np.dot(item_factors_with_properties_normalized, item_factors_with_properties_normalized.T)\n",
    "\n",
    "# Предсказываем 3 наиболее предпочтительных товара для заданного пользователя\n",
    "user_id = 'example_user_id'\n",
    "user_items = train[train['visitorid'] == user_id]['itemid'].tolist()\n",
    "\n",
    "# Вычисляем среднее значение факторной матрицы для пользователя\n",
    "user_factors = np.mean(item_factors_with_properties[train['itemid'].isin(user_items)], axis=0)\n",
    "\n",
    "# Вычисляем сходство между пользователем и всеми товарами\n",
    "user_item_similarity = np.dot(user_factors, item_factors_with_properties_normalized.T)\n",
    "\n",
    "# Сортируем товары по убыванию сходства и выбираем топ-3\n",
    "top_3_items_indices = np.argsort(user_item_similarity)[-3:]\n",
    "top_3_items = [(train_data.to_raw_iid(pred.iid), pred.est) for pred in test_data.testset.items if pred.iid in top_3_items_indices]\n",
    "print(\"Top 3 recommended items for user {}: {}\".format(user_id, top_3_items))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemeni\n",
    "from surprise import KNNWithMeans, Dataset, Reader\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "events = pd.read_csv(\"dfev.csv\")\n",
    "properties = pd.concat([\n",
    "    pd.read_csv(\"item_properties_part1.csv.zip\"),\n",
    "    pd.read_csv(\"item_properties_part2.csv.zip\")\n",
    "])\n",
    "# # Preprocess item properties\n",
    "# categorical_properties = [\"property1\", \"property2\", ...]  # Replace with actual categorical property names\n",
    "# numerical_properties = [\"property3\", \"property4\", ...]  # Replace with actual numerical property names\n",
    "\n",
    "# # One-hot encode categorical properties\n",
    "# ohe = OneHotEncoder(sparse=False)\n",
    "# encoded_categorical = ohe.fit_transform(properties_filtered[categorical_properties])\n",
    "\n",
    "# # Scale numerical properties\n",
    "# scaler = StandardScaler()\n",
    "# scaled_numerical = scaler.fit_transform(properties_filtered[numerical_properties])\n",
    "\n",
    "# # Combine encoded features\n",
    "# properties_features = pd.DataFrame(\n",
    "#     np.hstack([encoded_categorical, scaled_numerical]),\n",
    "#     columns=ohe.get_feature_names_out().tolist() + numerical_properties,\n",
    "#     index=properties_filtered.index,\n",
    "# )\n",
    "\n",
    "\n",
    "# Prepare data for Surprise\n",
    "transactions = events[events[\"event\"] == \"transaction\"]\n",
    "transactions[\"rating\"] = 1\n",
    "reader = Reader(rating_scale=(1, 1))\n",
    "data = Dataset.load_from_df(transactions[[\"visitorid\", \"itemid\", \"rating\"]].rename(columns={\"visitorid\": \"uid\", \"itemid\": \"iid\"}), reader)\n",
    "\n",
    "# # Integrate item properties\n",
    "# for item_id, features in properties_features.iterrows():\n",
    "#     data.add_item_features(item_id, features)\n",
    "\n",
    "# Build and train the model\n",
    "model = KNNWithMeans(sim_options={\"name\": \"cosine\", \"user_based\": True})\n",
    "model.fit(data.build_full_trainset())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
